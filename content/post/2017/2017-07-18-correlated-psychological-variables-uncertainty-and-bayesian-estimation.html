---
title: Correlated Psychological Variables, Uncertainty, and Bayesian Estimation
author: Matti Vuorre
date: '2017-07-18'
slug: correlated-psychological-variables-uncertainty-and-bayesian-estimation
categories:
  - psychology
tags:
  - statistics
  - psychology
  - bayes
  - modeling
  - R
  - research methods
output:
  blogdown::html_page:
    toc: true
    number_sections: false
    toc_depth: 2
draft: no
bibliography: ../../../static/bibliography/blog.bib
summary: "Assessing the correlations between psychological variabless, such as abilities and improvements, is one essential goal of psychological science. However, psychological variables are usually only available to the researcher as estimated parameters in mathematical and statistical models. The parameters are often estimated from small samples of observations for each research participant, which results in uncertainty (aka sampling error) about the participant-specific parameters. Ignoring the resulting uncertainty can lead to suboptimal inferences, such as asserting findings with too much confidence. Hierarchical models alleviate this problem by accounting for each parameter's uncertainty at the person- and average levels. However, common maximum likelihood estimation methods can have difficulties converging and finding appropriate values for parameters that describe the person-level parameters' spread and correlation. In this post, I discuss how Bayesian hierarchical models solve this problem, and advocate their use in estimating psychological variables and their correlations."
---


<div id="TOC">
<ul>
<li><a href="#psychological-variables">Psychological Variables</a></li>
<li><a href="#example-data">Example data</a></li>
<li><a href="#possible-models">Possible models</a><ul>
<li><a href="#independent-generalized-linear-models">Independent Generalized Linear Models</a></li>
<li><a href="#hierarchical-glm">Hierarchical GLM</a></li>
<li><a href="#estimating-glmm-with-maximum-likelihood-methods">Estimating GLMM with Maximum Likelihood Methods</a></li>
<li><a href="#bayesian-estimation-of-glmm">Bayesian estimation of GLMM</a></li>
<li><a href="#comparing-models">Comparing models</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a><ul>
<li><a href="#update">Update</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>Assessing the correlations between psychological variabless, such as abilities and improvements, is one essential goal of psychological science. However, psychological variables are usually only available to the researcher as estimated parameters in mathematical and statistical models. The parameters are often estimated from small samples of observations for each research participant, which results in uncertainty (aka sampling error) about the participant-specific parameters. Ignoring the resulting uncertainty can lead to suboptimal inferences, such as asserting findings with too much confidence. Hierarchical models alleviate this problem by accounting for each parameter’s uncertainty at the person- and average levels. However, common maximum likelihood estimation methods can have difficulties converging and finding appropriate values for parameters that describe the person-level parameters’ spread and correlation. In this post, I discuss how Bayesian hierarchical models solve this problem, and advocate their use in estimating psychological variables and their correlations.</p>
<div id="psychological-variables" class="section level1">
<h1>Psychological Variables</h1>
<p>Suppose you have conducted a study investigating how meditation affects people’s mood over Time. You asked 20 volunteers to fill a 10 item questionnaire once per week. They filled the questionnaire before starting a 3 week course in meditation—the baseline assessment of Mood—and then 3 times after each weekly meditation session. There are then (at least) two psychological variables of interest: People’s baseline mood, and the change in mood over time. These psychological variables can be operationalized as <em>parameters</em> in mathematical models—such as <em>t</em>-test, ANOVA, structural equation model—which are then statistically estimated from data. Furthermore, research questions may arise about the <em>correlations</em> between these parameters: For example, do people who start with a lower mood improve more?</p>
<p>Traditionally, the model estimation and parameter correlation steps are done separately, such as in common ANOVA and regression frameworks. In this post I would like to discuss why that is a bad idea, and show how <em>hierarchical models</em>—specifically when estimated with Bayesian methods—improve upon this method. My goal is not to provide a rigorous mathematical treatment of this problem, which is well documented in the literature <span class="citation">(Gelman and Hill 2007; Rouder et al. 2003)</span>. Instead, I hope to present an advanced-introductory-level treatment of the issues, focusing on visualizing the problems and solutions.</p>
</div>
<div id="example-data" class="section level1">
<h1>Example data</h1>
<p>Our hypothetical questionnaire consists of 10 yes/no questions that assess participants’ mood. Each yes (coded as 1) answer indicates a positive mood response to one questionnaire item. As usual, we have collected (simulated, this time!) the data and saved it in a file in the long format, shown below.</p>
<table>
<caption><span id="tab:data-table">Table 1: </span>First six rows of the example data</caption>
<thead>
<tr class="header">
<th align="right">id</th>
<th align="right">Time</th>
<th align="right">Mood</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>We can then visualize the data to see possible trends: How did people feel in the beginning of the study? Did their mood improve over time? First, Figure <a href="#fig:plot-data">1</a> shows that most people’s mood was near 0.5 in the beginning of the study—neither happy or unhappy. Second, the figure seems to suggest that at least some people’s mood improved over time (perhaps because they participated in the meditation course—we forgot to include a control group in the hypothetical study’s design!)</p>
<div class="figure"><span id="fig:plot-data"></span>
<img src="/post/2017/2017-07-18-correlated-psychological-variables-uncertainty-and-bayesian-estimation_files/figure-html/plot-data-1.png" alt="Each participant's mean Mood rating for each of the four time points." width="672" />
<p class="caption">
Figure 1: Each participant’s mean Mood rating for each of the four time points.
</p>
</div>
<p>However, there is also an implicit third dimension in these data: It is possible that people whose initial mood was quite high did not benefit from meditation, whereas those who started with a low mood might have improved more. This phenomenon is sometimes called a ceiling effect—distinguishing it from regression to the mean is sometimes difficult, but I won’t talk about that here. We would therefore also like to know if there is evidence of a ceiling effect in these data. But how do we quantify such effects? It seems difficult to include a parameter for such an effect in our statistical model (some regression of mood on time, for each participant). Let’s take a look at how we could do this.</p>
</div>
<div id="possible-models" class="section level1">
<h1>Possible models</h1>
<p>Our goal is to model these data to answer the following questions:</p>
<ol style="list-style-type: decimal">
<li>What were people’s baseline moods? How did they feel at week 0? What was the average baseline mood?</li>
<li>How did people’s mood change over time? Did they improve? What was the average change?</li>
<li>Did those who improved least begin with the highest mood? Was there a ceiling or floor effect? <em>Were the baselines and changes correlated?</em></li>
</ol>
<p>At this point the analyst faces a series of options (the garden of forking paths; <span class="citation">Gelman and Loken (2014)</span>), each of which could lead to different data being passed to a different statistical model. One common option, ANOVA, would assume normally distributed data, and is excluded from consideration immediately, because we know that people answered binary questions. Furthermore, ANOVA would require collapsing the data to cell means for every participant, in order to pass the proportions to the ANOVA model. It would be more appropriate to account for the raw data with a model that respects the fact that the data are binary outcomes, and multiply measured over individuals. Such a model could be constructed, separately for each individual, as a Generalized Linear Model, such as a logistic regression:</p>
<p>\begin{aligned}
 &amp;Bernoulli(p) \
p &amp;= () \
&amp;= u_0 + u_1
\end{aligned}</p>
<p>These equations specify that each mood rating is a random draw from a Bernoulli distribution (Binomial distribution with one trial) with probability parameter <em>p</em>. The probability parameter is an inverse-logit transformation of a linear predictor <span class="math inline">\(\eta\)</span> (eta). <span class="math inline">\(\eta\)</span>, in turn, is just a sum of that person’s intercept <span class="math inline">\(u_0\)</span> and coefficient for time <span class="math inline">\(u_1\)</span> (multiplied by the week of observation). For this exposition I am ignoring all subscripts for clarity.</p>
<div id="independent-generalized-linear-models" class="section level2">
<h2>Independent Generalized Linear Models</h2>
<p>When conducting regression analyses on repeated measures data, an old practice was to fit the model separately for each individual <span class="citation">(Lorch and Myers 1990)</span>. More than a recommendation, this was often a computational necessity because other methods simply weren’t available, or were not known in some fields of study. This model would result in 20 intercepts (<span class="math inline">\(u_0\)</span>) and slopes (<span class="math inline">\(u_1\)</span>), one for each individual, shown in Table <a href="#tab:glm-coefs">2</a>.</p>
<table>
<caption><span id="tab:glm-coefs">Table 2: </span>Regression coefficients from independently fitted GLMs</caption>
<thead>
<tr class="header">
<th align="right">id</th>
<th align="right">(Intercept)</th>
<th align="right">Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.529</td>
<td align="right">0.137</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.488</td>
<td align="right">-0.123</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.895</td>
<td align="right">0.050</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">-0.324</td>
<td align="right">0.285</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.706</td>
<td align="right">0.095</td>
</tr>
</tbody>
</table>
<p>We could then solve the first and second problems by summarizing these participant-specific coefficients and testing them against zero with, say, <em>t</em>-tests. Note that this would imply fitting as many models as there are subjects, and then two more tests (one for each t-test) to assess the average-level “pseudoparameters” (mean Intercept, for example). We can intuit that fitting many models might lead to problems with multiple comparisons, false alarms, all that fun stuff. I visualize these person-level estimates in Figure <a href="#fig:independent-parameters-plot-1">2</a>, where the average-level Intercept and Time coefficients are drawn as blue dots with bootstrapped 95%CIs.</p>
<div class="figure"><span id="fig:independent-parameters-plot-1"></span>
<img src="/post/2017/2017-07-18-correlated-psychological-variables-uncertainty-and-bayesian-estimation_files/figure-html/independent-parameters-plot-1-1.png" alt="Each participant provides one data point (his or her parameter) to the average-level estimate of each parameter. The average-level parameters therefore ignore the uncertainty in the person-specific estimates. Boo!" width="384" />
<p class="caption">
Figure 2: Each participant provides one data point (his or her parameter) to the average-level estimate of each parameter. The average-level parameters therefore ignore the uncertainty in the person-specific estimates. Boo!
</p>
</div>
<p>However, an additional problem arises when we focus on the third question, the possible correlation between these parameters. Although it might seem a good idea to simply correlate the intercepts and slopes, the resulting estimate of the correlation coefficient would ignore the fact that each participant-specific intercept and slope has its own associated uncertainty (standard error). Ignoring the uncertainty, in turn, could thereby produce a too narrow standard error for the correlation coefficient! Consequently, we might then assert the (non-)existence of ceiling or floor effects with too much confidence. A second problem would be that if the data were unbalanced (some people had only a few observations, others many more), the model would still weight each person’s single datapoint equally.</p>
<div class="figure"><span id="fig:independent-parameters-plot-2"></span>
<img src="/post/2017/2017-07-18-correlated-psychological-variables-uncertainty-and-bayesian-estimation_files/figure-html/independent-parameters-plot-2-1.png" alt="A simple correlation would ignore the error bars associated with each estimated parameter. Error bars are 1 SEM." width="384" />
<p class="caption">
Figure 3: A simple correlation would ignore the error bars associated with each estimated parameter. Error bars are 1 SEM.
</p>
</div>
<p>In Figure <a href="#fig:independent-parameters-plot-2">3</a> I draw a scatterplot of the person-specific Intercepts and Time slopes with error bars representing 1 standard error of the mean. Notice how large those error bars are! We wouldn’t want to ignore those error bars. How, then, do we construct a model that appropriately takes into account these uncertainties?</p>
</div>
<div id="hierarchical-glm" class="section level2">
<h2>Hierarchical GLM</h2>
<p>The “multiple-models” problem described above (ignoring the uncertainty in the person-level parameters, then using these parameters at a second level of analysis) is solved by hierarchical models (e.g. <span class="citation">Gelman and Hill (2007)</span>, sometimes called multilevel models or mixed models). These models include both person-specific parameters (<span class="math inline">\(u_0\)</span>s and <span class="math inline">\(u_1\)</span>s from above), and average (or “population-level”) parameters in one model. Essentially, the person-specific parameters are modelled as random draws from a multivariate Normal distribution, whose parameters are the average parameters, and the person-specific effects’ variances and covariances. These models therefore have parameters to answer all of our three problems.</p>
<p>We can simply expand the subject-specific model given above into a hierarchical model, which I will henceforth call a GLMM (Generalized Linear Mixed Model). In the same notation as above, this time including subscripts for person <em>j</em> and row <em>i</em> in the data, the model is now:</p>
<p>\begin{aligned}
 &amp;Bernoulli(p_{ji}) \
p_{ji} &amp;= (<em>{ji}) \
</em>{ji} &amp;= u_{0j} + u_{1j} Time_{ji}
\end{aligned}</p>
<p>The person-specific effects (<em>u</em>s) are modeled as multivariate normal with two means <span class="math inline">\(\beta_0\)</span> (average intercept) and <span class="math inline">\(\beta_1\)</span> (average change in mood) and covariance matrix <span class="math inline">\(\Sigma\)</span> (capital sigma).</p>
<p><span class="math display">\[
\left[\begin{array}{c}
u_{0j} \\ u_{1j}
\end{array}\right] 
\sim N( 
\left[\begin{array}{c}
\beta_0 \\ \beta_1
\end{array}\right], 
\Sigma)
\]</span></p>
<p>This model therefore answers all our three questions in one simultaneous step of inference:</p>
<ol style="list-style-type: decimal">
<li>Baseline mood, how did people feel at week 0.
<ul>
<li><span class="math inline">\(\beta_0\)</span> for the average person</li>
<li><span class="math inline">\(u_{0j}\)</span> for person <em>j</em></li>
</ul></li>
<li>Mood improvement over time.
<ul>
<li><span class="math inline">\(\beta_1\)</span> for the average person</li>
<li><span class="math inline">\(u_{1j}\)</span> for person <em>j</em></li>
</ul></li>
<li>Possible ceiling effect: Did those who improve least begin with the highest mood?
<ul>
<li><span class="math inline">\(\rho\)</span> (correlation between <span class="math inline">\(u_{0j}\)</span>s and <span class="math inline">\(u_{1j}\)</span>)</li>
</ul></li>
</ol>
<p>The correlation parameter <span class="math inline">\(\rho\)</span> can be obtained from <span class="math inline">\(\Sigma\)</span>, and directly quantifies the extent to which people’s baseline moods (<span class="math inline">\(u_0\)</span>s) covary with mood improvements (<span class="math inline">\(u_1\)</span>s). Essentially, if we find a strongly negative <span class="math inline">\(\rho\)</span>, we should be inclined to believe that individuals with higher baseline moods are less affected by the meditation exercises.</p>
</div>
<div id="estimating-glmm-with-maximum-likelihood-methods" class="section level2">
<h2>Estimating GLMM with Maximum Likelihood Methods</h2>
<p>These multilevel models are most commonly estimated with Maximum Likelihood Methods, such as ones implemented in the popular R package <strong>lme4</strong> <span class="citation">(Bates et al. 2015)</span>.</p>
<pre class="r"><code>mle &lt;- glmer(Mood ~ Time + (Time|id), family=binomial(), data = d)</code></pre>
<p>From this model, we can then obtain the <em>empirical Bayes</em><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> estimates of <span class="math inline">\(u_0\)</span>s and <span class="math inline">\(u_1\)</span>s</p>
<div class="figure"><span id="fig:plot-mles"></span>
<img src="/post/2017/2017-07-18-correlated-psychological-variables-uncertainty-and-bayesian-estimation_files/figure-html/plot-mles-1.png" alt="Person-specific intercept (X-axis) and slope (Y-axis) parameters obtained with maximum likelihood methods." width="384" />
<p class="caption">
Figure 4: Person-specific intercept (X-axis) and slope (Y-axis) parameters obtained with maximum likelihood methods.
</p>
</div>
<p>We can clearly see that something odd has happened with our attempt to estimate the person-specific parameters. In Figure <a href="#fig:plot-mles">4</a>, the two person-specific parameters are perfectly correlated. This happens because the empirical Bayes estimates are conditional on components of <span class="math inline">\(\Sigma\)</span>, and MLE methods can have difficulties in estimating these components, such as <span class="math inline">\(\rho\)</span>. Figure <a href="#fig:profile-plot">5</a> shows the model’s estimated profile for this correlation parameter: There appears a significant peak at <span class="math inline">\(\rho = -1\)</span>, and because MLE works with this “best estimate” and produces person-specific effects <span class="math inline">\(u_0\)</span>s and <span class="math inline">\(u_1\)</span>s conditional on <span class="math inline">\(\rho = -1\)</span>, the person-specific effects are perfectly negatively correlated in Figure <a href="#fig:plot-mles">4</a>.</p>
<div class="figure"><span id="fig:profile-plot"></span>
<img src="/post/2017/2017-07-18-correlated-psychological-variables-uncertainty-and-bayesian-estimation_files/figure-html/profile-plot-1.png" alt="Likelihood profile of $\rho$, the parameter quantifying the correlation between person-specific Time and Intercept parameters. On the X axis are possible parameter values (this parameter must be between -1 and 1), and the Y axis shows the likelihood profile over these values." width="384" />
<p class="caption">
Figure 5: Likelihood profile of <span class="math inline">\(\rho\)</span>, the parameter quantifying the correlation between person-specific Time and Intercept parameters. On the X axis are possible parameter values (this parameter must be between -1 and 1), and the Y axis shows the likelihood profile over these values.
</p>
</div>
<p>One way to describe this situation is to say that some components of <span class="math inline">\(\Sigma\)</span> have collapsed to a boundary value, such as 0 (for the standard deviations) or -1 or 1 for <span class="math inline">\(\rho\)</span>. This happens because the MLE methods target point estimates for these parameters, which may be at boundary. They therefore ignore the fact that the likelihood (or posterior distribution) may be wide and contain plausible values far from the boundary. In fact, you can see from Figure <a href="#fig:profile-plot">5</a> that something odd is happening, because the profile is not smoothly estimated. From the output below, we can see that this time, MLE has estimated <span class="math inline">\(\rho\)</span> (<code>Corr</code>) to be -1, therefore leading the person-specific parameters to form a perfect line.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<pre><code>##  Groups Name        Std.Dev. Corr  
##  id     (Intercept) 0.386371       
##         Time        0.044011 -1.000</code></pre>
<p>So, although MLE methods took the uncertainty of the person-level parameters into account, they failed to provide reasonable estimates. Here’s where Bayesian estimation, by way of using the entire posterior distribution of compenents of <span class="math inline">\(\Sigma\)</span> while estimating the <em>u</em>s, is especially helpful.</p>
</div>
<div id="bayesian-estimation-of-glmm" class="section level2">
<h2>Bayesian estimation of GLMM</h2>
<p>Fortunately, Bayesian estimation of GLMMs is now made easy with various R packages, such as brms <span class="citation">(Buerkner 2016)</span>. The syntax is very similar to the above lme4 one:</p>
<pre class="r"><code>bay &lt;- brm(Mood ~ Time + (Time|id), family=bernoulli(), data = d, cores=4,
           file = here::here(&quot;static/data/moodtimemodel&quot;))</code></pre>
<p>Once the model is estimated, we can then look at the estimated components of <span class="math inline">\(\Sigma\)</span>:</p>
<table>
<caption><span id="tab:estimate-bayes-2">Table 3: </span>Estimated parameters (summaries of their posterior distributions) from the model fitted with Bayesian methods.</caption>
<thead>
<tr class="header">
<th align="left">Parameter</th>
<th align="right">Estimate</th>
<th align="right">Est.Error</th>
<th align="right">l-95% CI</th>
<th align="right">u-95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">sd(Intercept)</td>
<td align="right">0.39</td>
<td align="right">0.16</td>
<td align="right">0.10</td>
<td align="right">0.74</td>
</tr>
<tr class="even">
<td align="left">sd(Time)</td>
<td align="right">0.09</td>
<td align="right">0.07</td>
<td align="right">0.00</td>
<td align="right">0.25</td>
</tr>
<tr class="odd">
<td align="left">cor(Intercept,Time)</td>
<td align="right">-0.20</td>
<td align="right">0.55</td>
<td align="right">-0.96</td>
<td align="right">0.91</td>
</tr>
</tbody>
</table>
<p>Instead of the point estimates dealt to us by the MLE methods above, we now have posterior distributions for the two standard deviations and one correlation parameter. We can visualize these three parameters using the Bayesplot R package <span class="citation">(Gabry 2017)</span>:</p>
<div class="figure"><span id="fig:mcmc-areas"></span>
<img src="/post/2017/2017-07-18-correlated-psychological-variables-uncertainty-and-bayesian-estimation_files/figure-html/mcmc-areas-1.png" alt="Smoothed density plots of the posterior distributions of the two Standard Deviations and one Correlation parameter inside $\Sigma$. Notice that the correlation parameter is very wide, reflecting great uncertainty in the underlying values. The vertical bars are posterior medians, and shaded areas are 80\% Credibility Intervals." width="384" />
<p class="caption">
Figure 6: Smoothed density plots of the posterior distributions of the two Standard Deviations and one Correlation parameter inside <span class="math inline">\(\Sigma\)</span>. Notice that the correlation parameter is very wide, reflecting great uncertainty in the underlying values. The vertical bars are posterior medians, and shaded areas are 80% Credibility Intervals.
</p>
</div>
<p>We can now also visualize the person-specific intercepts and slopes. Figure <a href="#fig:bay-coefs">7</a> shows that the person-specific parameters are no longer perfectly correlated.</p>
<div class="figure"><span id="fig:bay-coefs"></span>
<img src="/post/2017/2017-07-18-correlated-psychological-variables-uncertainty-and-bayesian-estimation_files/figure-html/bay-coefs-1.png" alt="Person-specific parameters as estimated in the Bayesian GLMM. Note that each point is the posterior mean." width="384" />
<p class="caption">
Figure 7: Person-specific parameters as estimated in the Bayesian GLMM. Note that each point is the posterior mean.
</p>
</div>
<p>With these much more realistic estimates, which also take the uncertainty at the person-level into account, we can now finally answer the third question: Did people who started with a higher mood benefit less from the meditation course? The answer to this question is given by <span class="math inline">\(\rho\)</span> (<code>cor(Intercept,Time)</code>) in Table <a href="#tab:estimate-bayes-2">3</a>. Although the posterior mean is negative, the 95% Credibility Interval spans almost the entire range of values from -1 to 1. Figure <a href="#fig:bay-coefs">7</a> drives home the same point, but in visual form: Our best guess is that people with higher baseline moods benefit less from meditation, but we are very uncertain in this guess.</p>
</div>
<div id="comparing-models" class="section level2">
<h2>Comparing models</h2>
<p>Finally, let’s visualize the person-specific parameters from all three methods, side-by-side. First, Figure <a href="#fig:compare-coefs">8</a> shows that the independent models method led to great dispersion among the individual-specific estimates. Although each of these estimates reflects only that person’s data, they ignore the fact that the people themselves are sampled from a population of people. Therefore, it is commonly argued that information should be somehow pooled across individuals, such that the most extreme values would be attenuated toward the estimated population means <span class="citation">(Gelman and Hill 2007; Gelman, Hill, and Yajima 2012; Rouder et al. 2003)</span>. This is essentially what the two panels on the right show, the person-specific estimates are “shrunk” toward their means and therefore less dispersed. “People are different, but not <em>that</em> different.”</p>
<p>It is argued that Bayesian shrinkage improves the estimates of psychological variables <span class="citation">(Rouder et al. 2003)</span>. Although I agree, that topic is outside the scope of this post. However, I would simply like to point out that while the MLE estimates are shrunk, they are estimated conditional on the correlation parameter being estimated at the boundary (-1). The Bayesian estimates, on the other hand, experience shrinkage but have no issue of being conditionalized on a boundary value. Consequently, the should better reflect the true baseline moods and changes of the individuals in our study.</p>
<div class="figure"><span id="fig:compare-coefs"></span>
<img src="/post/2017/2017-07-18-correlated-psychological-variables-uncertainty-and-bayesian-estimation_files/figure-html/compare-coefs-1.png" alt="Person-specific intercepts (baseline mood) and Time-coefficients (magnitude of change) for the three candidate models. Note that both parameters are on the log-odds scale." width="672" />
<p class="caption">
Figure 8: Person-specific intercepts (baseline mood) and Time-coefficients (magnitude of change) for the three candidate models. Note that both parameters are on the log-odds scale.
</p>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Hopefully, this post has developed some visual intuition to motivate the widespread use of (Bayesian) hierarchical models, and understand their underlying logic. Hierarchical (Bayesian) models are advocated for a variety of cognitive and descriptive models in the cognitive psychology literature <span class="citation">(Rouder et al. 2003, 2014, 2007; Rouder and Lu 2005)</span>, but have been somewhat difficult to implement in practice. However, with recent advances in computational algorithms <span class="citation">(Stan Development Team 2016)</span> and higher-level programming interfaces to these algorithms <span class="citation">(Buerkner 2016)</span>, hierarchical Bayesian models are now available to applied researchers with minimal time investments to learn the tools.</p>
<p>Consequently, I would argue that modelers (that’s you) should consider Bayesian methods as their default estimation method for models estimating what I’ve here called “Psychological variables” <span class="citation">(Kruschke and Liddell 2017; Kruschke 2010)</span>.</p>
<p>Finally, I would also like to point out an implication of the foregoing to social media arguments on Bayes vs. Frequentism. These arguments almost exclusively focus on comparing the properties of Bayes Factors to <em>p</em>-values. From the perspective of this post, focusing on that issue diverts attention from a more important implication of using Bayesian statistics, which is their superior ability to estimate complex statistical models. You can, and should (I argue), use Bayesian methods even if you don’t care about Bayes Factors, or are uninterested in the more philosophical differences between the two schools of thought (but interested parties may read <span class="citation">Jaynes (2003)</span>).</p>
<div id="update" class="section level2">
<h2>Update</h2>
<p>{{% alert note %}}
Priors are not what you think they are.
{{% /alert %}}</p>
<p>I received some very helpful feedback and questions from folks on Twitter. One general issue was what sort of <strong>priors</strong> were used in the Bayesian analysis:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
But this stuff is not problematic use of prior info, everyone will agree. Would be nice to read exactly what info it uses.
</p>
— Daniël Lakens (<span class="citation">(<span class="citeproc-not-found" data-reference-id="lakens"><strong>???</strong></span>)</span>) <a href="https://twitter.com/lakens/status/887519771859857409">July 19, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>The answer to Lakens’ query, “Would be nice to read exactly what info it uses.”: The estimates use info from the data, and nothing else (given the model’s other features). The key point is that these models can be estimated without any user-defined prior parameters. However, Frequentist and Bayesian estimation of GLMMs both set a structure on the person-level parameters <span class="math inline">\(u\)</span> in the form of a multivariate Gaussian. The covariance matrix of this Gaussian therefore contains “prior” information on the <span class="math inline">\(u\)</span>s, but the prior parameters are not set by the user. Instead, they are estimated from data—hence the label “Empirical Bayes”.</p>
<p>In the Bayesian framework, it is of course possible to set “hyper”priors on the elements of the covariance matrix. But here I didn’t<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. It is more important to recognize that any model structure that you, the analyst, define, is “prior information” in the sense that it influences the inference you’ll make. In other words, the likelihood makes a bigger difference than any priors, but is often less contested.</p>
<p>Alexander Etz was also quick to point out potential confusions in interpreting the “average-person” estimates <span class="math inline">\(\beta\)</span> in GLMMs, and posted links to helpful papers:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
You must be careful interpreting betas in glmms. The marginal effects are not equal to the conditional effects due to the nonlinear link
</p>
— Alexander Etz (<span class="citation">(<span class="citeproc-not-found" data-reference-id="AlxEtz"><strong>???</strong></span>)</span>) <a href="https://twitter.com/AlxEtz/status/887649038597849089">July 19, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
Some lit on this: <br>- <a href="https://t.co/wSi8PHJ2OK">https://t.co/wSi8PHJ2OK</a><br>- <a href="https://t.co/pTqN5dkF2l">https://t.co/pTqN5dkF2l</a><br>- <a href="https://t.co/WCcxcERpGX">https://t.co/WCcxcERpGX</a>
</p>
— Alexander Etz (<span class="citation">(<span class="citeproc-not-found" data-reference-id="AlxEtz"><strong>???</strong></span>)</span>) <a href="https://twitter.com/AlxEtz/status/887654821720989704">July 19, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Thanks!</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-bates_fitting_2015">
<p>Bates, Douglas M., Martin Mächler, Ben M. Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using Lme4.” <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01">https://doi.org/10.18637/jss.v067.i01</a>.</p>
</div>
<div id="ref-buerkner_brms:_2016">
<p>Buerkner, Paul-Christian. 2016. <em>Brms: Bayesian Regression Models Using Stan</em>. <a href="http://CRAN.R-project.org/package=brms">http://CRAN.R-project.org/package=brms</a>.</p>
</div>
<div id="ref-gabry_bayesplot:_2017">
<p>Gabry, Jonah. 2017. <em>Bayesplot: Plotting for Bayesian Models</em>. <a href="http://mc-stan.org/">http://mc-stan.org/</a>.</p>
</div>
<div id="ref-gelman_data_2007">
<p>Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge University Press.</p>
</div>
<div id="ref-gelman_why_2012">
<p>Gelman, Andrew, Jennifer Hill, and Masanao Yajima. 2012. “Why We (Usually) Don’t Have to Worry About Multiple Comparisons.” <em>Journal of Research on Educational Effectiveness</em> 5 (2): 189–211. <a href="https://doi.org/10.1080/19345747.2011.618213">https://doi.org/10.1080/19345747.2011.618213</a>.</p>
</div>
<div id="ref-gelman_statistical_2014">
<p>Gelman, Andrew, and Eric Loken. 2014. “The Statistical Crisis in Science.” <em>American Scientist</em> 102 (6): 460. <a href="https://doi.org/10.1511/2014.111.460">https://doi.org/10.1511/2014.111.460</a>.</p>
</div>
<div id="ref-jaynes_probability_2003">
<p>Jaynes, E. T. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge University Press.</p>
</div>
<div id="ref-kruschke_what_2010">
<p>Kruschke, John K. 2010. “What to Believe: Bayesian Methods for Data Analysis.” <em>Trends in Cognitive Sciences</em> 14 (7): 293–300. <a href="https://doi.org/10.1016/j.tics.2010.05.001">https://doi.org/10.1016/j.tics.2010.05.001</a>.</p>
</div>
<div id="ref-kruschke_bayesian_2017-1">
<p>Kruschke, John K., and Torrin M. Liddell. 2017. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” <em>Psychonomic Bulletin &amp; Review</em>, February, 1–29. <a href="https://doi.org/10.3758/s13423-016-1221-4">https://doi.org/10.3758/s13423-016-1221-4</a>.</p>
</div>
<div id="ref-lorch_regression_1990">
<p>Lorch, Robert F., and Jerome L. Myers. 1990. “Regression Analyses of Repeated Measures Data in Cognitive Research.” <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 16 (1): 149. <a href="http://psycnet.apa.org/journals/xlm/16/1/149/">http://psycnet.apa.org/journals/xlm/16/1/149/</a>.</p>
</div>
<div id="ref-rouder_introduction_2005">
<p>Rouder, Jeffrey N., and Jun Lu. 2005. “An Introduction to Bayesian Hierarchical Models with an Application in the Theory of Signal Detection.” <em>Psychonomic Bulletin &amp; Review</em> 12 (4): 573–604. <a href="https://doi.org/10.3758/BF03196750">https://doi.org/10.3758/BF03196750</a>.</p>
</div>
<div id="ref-rouder_signal_2007">
<p>Rouder, Jeffrey N., Jun Lu, Dongchu Sun, Paul Speckman, Richard D. Morey, and Moshe Naveh-Benjamin. 2007. “Signal Detection Models with Random Participant and Item Effects.” <em>Psychometrika</em> 72 (4): 621–42. <a href="https://doi.org/10.1007/s11336-005-1350-6">https://doi.org/10.1007/s11336-005-1350-6</a>.</p>
</div>
<div id="ref-rouder_lognormal_2014">
<p>Rouder, Jeffrey N., Jordan M. Province, Richard D. Morey, Pablo Gomez, and Andrew Heathcote. 2014. “The Lognormal Race: A Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties.” <em>Psychometrika</em> 80 (2): 491–513. <a href="https://doi.org/10.1007/s11336-013-9396-3">https://doi.org/10.1007/s11336-013-9396-3</a>.</p>
</div>
<div id="ref-rouder_hierarchical_2003">
<p>Rouder, Jeffrey N., Dongchu Sun, Paul L. Speckman, Jun Lu, and Duo Zhou. 2003. “A Hierarchical Bayesian Statistical Framework for Response Time Distributions.” <em>Psychometrika</em> 68 (4): 589–606. <a href="https://doi.org/10.1007/BF02295614">https://doi.org/10.1007/BF02295614</a>.</p>
</div>
<div id="ref-stan_development_team_rstan:_2016">
<p>Stan Development Team. 2016. <em>RStan: The R Interface to Stan</em>. <a href="http://mc-stan.org/">http://mc-stan.org/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>They are called empirical Bayes estimates because each subject’s estimates inform other subjects’ estimates via the shared parameters of the upper level distribution (<span class="math inline">\(\beta\)</span>s and components of <span class="math inline">\(\Sigma\)</span>).<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This problem is especially salient when working with Generalized Linear Models, instead of Gaussian models. However, I don’t wish to overstate the problem, or in any way suggest that there is a problem in how these methods work, including the <strong>lme4</strong> R package. The data for this example are simulated to produce this exact problem, and there are steps one could take to alleviate it: For example, you could adjust the options of the <code>glmer()</code> function.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>brms sets reasonable (pretty much non-informative) defaults for these, which can be taken out. For the purposes of this post, these defaults can be entirely ignored.<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
