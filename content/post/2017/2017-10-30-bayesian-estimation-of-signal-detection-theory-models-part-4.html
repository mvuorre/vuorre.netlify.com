---
title: Bayesian Estimation of Signal Detection Models, Part 4
author: Matti Vuorre
date: '2017-10-30'
slug: bayesian-estimation-of-signal-detection-theory-models-part-4
categories:
  - psychology
  - statistics
tags:
  - bayes
  - brms
  - modeling
  - psychology
  - R
  - statistics
  - tutorial
draft: no
output:
  blogdown::html_page:
    toc: yes
    number_sections: no
    toc_depth: 2
    df_print: paged
summary: (This post is part 4 of a series of blog posts discussing Bayesian estimation of Signal Detection models.) In this blog post, I describe how to estimate the unequal variances Gaussian signal detection (UVSDT) model for confidence rating responses, for multiple participants simultaneously. I provide software code for the hierarchical Bayesian model in R.
bibliography: "/Users/Matti/Documents/vuorre.netlify.com/static/bibliography/blog.bib"
---

<link href="/rmarkdown-libs/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="/rmarkdown-libs/pagedtable/js/pagedtable.js"></script>

<div id="TOC">
<ul>
<li><a href="#hierarchical-uvsdt-model">Hierarchical UVSDT model</a><ul>
<li><a href="#example-data-set">Example data set</a></li>
<li><a href="#model-syntax">Model syntax</a></li>
<li><a href="#prior-distributions">Prior distributions</a></li>
<li><a href="#estimate-and-summarise-parameters">Estimate and summarise parameters</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>This post is the fourth part in a series of blog posts on Signal Detection models <span class="citation">(Macmillan and Creelman 2005; McNicol 2005)</span>. In the first part, I described how to estimate the equal variance Gaussian SDT (EVSDT) model for a single participant, using Bayesian (generalized linear and nonlinear) modeling techniques. In the second part, I described how to estimate the equal variance Gaussian SDT model for multiple participants simultaneously, using hierarchical Bayesian models. In the third post, I showed how to esimate equal- and unequal variances SDT models for confidence rating data for a single participant.</p>
<p>However, we almost always want to discuss our inference about the population, not individual subjects. Further, if we wish to discuss individual subjects, we should place them in the context of other subjects. A multilevel (aka hierarchical, mixed) model accomplishes these goals by including population- and subject-level parameters.</p>
<p>I will again describe the software implementation in R using the brms package <span class="citation">(Bürkner 2017; R Core Team 2017)</span>. This blog post will be shorter than the previous installments; I assume you’re familiar with the material covered in those posts.</p>
<div id="hierarchical-uvsdt-model" class="section level1">
<h1>Hierarchical UVSDT model</h1>
<div id="example-data-set" class="section level2">
<h2>Example data set</h2>
<p>We’ll use a data set of 48 subjects’ confidence ratings on a 6 point scale: 1 = “sure new”, …, 6 = “sure old” <span class="citation">(Koen et al. 2013)</span>. This data set is included in the R package MPTinR <span class="citation">(Singmann and Kellen 2013)</span>.</p>
<p>In this experiment <span class="citation">(Koen et al. 2013)</span>, participants completed a study phase, and were then tested under full attention, or while doing a second task. Here, we focus on the rating data provided in the full attention condition. Below, I reproduce the aggregate rating counts for old and new items from the Table in the article’s appendix. (It is useful to ensure that we are indeed using the same data.)</p>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["isold"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["6"],"name":[2],"type":["int"],"align":["right"]},{"label":["5"],"name":[3],"type":["int"],"align":["right"]},{"label":["4"],"name":[4],"type":["int"],"align":["right"]},{"label":["3"],"name":[5],"type":["int"],"align":["right"]},{"label":["2"],"name":[6],"type":["int"],"align":["right"]},{"label":["1"],"name":[7],"type":["int"],"align":["right"]}],"data":[{"1":"old","2":"2604","3":"634","4":"384","5":"389","6":"422","7":"309"},{"1":"new","2":"379","3":"356","4":"454","5":"871","6":"1335","7":"1365"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>For complete R code, including pre-processing the data, please refer to the <a href="https://github.com/mvuorre/blog">source code</a> of this blog post. I have omitted some of the less important code from the blog post for clarity.</p>
</div>
<div id="model-syntax" class="section level2">
<h2>Model syntax</h2>
<p>Here’s the brms syntax we used for estimating the model for a single participant:</p>
<pre class="r"><code>uvsdt_m &lt;- bf(y ~ isold, disc ~ 0 + isold)</code></pre>
<p>With the above syntax we specifed seven parameters: Five intercepts (aka ‘thresholds’ in the cumulative probit model) on <code>y</code><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>; the effect of <code>isold</code> on <code>y</code>; and the effect of <code>isold</code> on the discrimination parameter <code>disc</code><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. There are five intercepts (thresholds), because there are six response categories.</p>
<p>We extend the code to a hierarchical model by specifying that all these parameters vary across participants (variable <code>id</code> in the data).</p>
<pre class="r"><code>uvsdt_h &lt;- bf(y ~ isold + (isold |s| id), 
              disc ~ 0 + isold + (0 + isold |s| id))</code></pre>
<p>Recall from earlier posts that using <code>|s|</code> leads to estimating correlations among the varying effects. There will only be one standard deviation associated with the thresholds; that is, the model assumes that subjects vary around the mean threshold similarly for all thresholds.</p>
</div>
<div id="prior-distributions" class="section level2">
<h2>Prior distributions</h2>
<p>I set a N(1, 3) prior on dprime, just because I know that in these tasks performance is usually pretty good. Perhaps this prior is also influenced by my reading of the paper! I also set a N(0, 1) prior on <em>a</em>: Usually this parameter is found to be around <span class="math inline">\(-\frac{1}{4}\)</span>, but I’m ignoring that information.</p>
<p>The <em>t(7, 0, .33)</em> priors on the between-subject standard deviations reflect my assumption that the subjects should be moderately similar to one another, but also allows larger deviations. (They are <em>t</em>-distributions with seven degrees of freedom, zero mean, and .33 standard deviation.)</p>
<pre class="r"><code>Prior &lt;- c(prior(normal(1, 3), class = &quot;b&quot;, coef = &quot;isold&quot;),
           prior(normal(0, 1), class = &quot;b&quot;, coef = &quot;isold&quot;, dpar = &quot;disc&quot;),
           prior(student_t(7, 0, .33), class = &quot;sd&quot;),
           prior(student_t(7, 0, .33), class = &quot;sd&quot;, dpar = &quot;disc&quot;),
           prior(lkj(2), class = &quot;cor&quot;))</code></pre>
</div>
<div id="estimate-and-summarise-parameters" class="section level2">
<h2>Estimate and summarise parameters</h2>
<p>We can then estimate the model as before. Be aware that this model takes quite a bit longer to estimate (note that I have reduced the iterations to 500 from the default 2000).</p>
<pre class="r"><code>fit &lt;- brm(uvsdt_h,
           family = cumulative(link=&quot;probit&quot;),
           data = d,
           prior = Prior,
           control = list(adapt_delta = .9), inits = 0,
           cores = 4, iter = 500,
           file = here::here(&quot;static/data/sdtmodel4-1&quot;))</code></pre>
<p>We then display numerical summaries of the model’s parameters. Note that the effective sample sizes are modest, and Rhats indicate that we would benefit from drawing more samples from the posterior. For real applications, I recommend more than 500 iterations per chain.</p>
<pre class="r"><code>summary(fit)</code></pre>
<pre><code>##  Family: cumulative 
##   Links: mu = probit; disc = log 
## Formula: y ~ isold + (isold | s | id) 
##          disc ~ 0 + isold + (0 + isold | s | id)
##    Data: d (Number of observations: 9502) 
## Samples: 4 chains, each with iter = 500; warmup = 250; thin = 1;
##          total post-warmup samples = 1000
## 
## Group-Level Effects: 
## ~id (Number of levels: 48) 
##                           Estimate Est.Error l-95% CI u-95% CI Eff.Sample
## sd(Intercept)                 0.35      0.04     0.28     0.43        210
## sd(isold)                     0.79      0.10     0.62     1.01        321
## sd(disc_isold)                0.46      0.05     0.38     0.55        319
## cor(Intercept,isold)         -0.47      0.12    -0.68    -0.22        121
## cor(Intercept,disc_isold)     0.35      0.13     0.08     0.58        273
## cor(isold,disc_isold)        -0.76      0.07    -0.87    -0.61        455
##                           Rhat
## sd(Intercept)             1.03
## sd(isold)                 1.00
## sd(disc_isold)            1.01
## cor(Intercept,isold)      1.03
## cor(Intercept,disc_isold) 1.02
## cor(isold,disc_isold)     1.00
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept[1]    -0.60      0.05    -0.70    -0.49        200 1.00
## Intercept[2]     0.20      0.05     0.09     0.30        197 1.00
## Intercept[3]     0.69      0.05     0.60     0.80        203 1.00
## Intercept[4]     1.04      0.05     0.94     1.14        212 1.01
## Intercept[5]     1.49      0.05     1.39     1.59        230 1.00
## isold            1.86      0.12     1.63     2.10        119 1.04
## disc_isold      -0.38      0.07    -0.52    -0.25        112 1.05
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Let’s first focus on the “Population-level Effects”: The effects for the “average person”. <code>isold</code> is <em>d’</em>, and is very close to the one reported in the paper (eyeballing Figure 3 in <span class="citation">Koen et al. (2013)</span>; this <em>d’</em> is not numerically reported in the paper). <code>disc_isold</code> is, because of the model’s parameterization, <span class="math inline">\(-\mbox{log}(\sigma_{signal}) = -a\)</span>. The paper discusses <span class="math inline">\(V_o = \sigma_{signal}\)</span>, and therefore we transform each posterior sample of our <em>-a</em> to obtain samples from <span class="math inline">\(V_o\)</span>’s posterior distribution.</p>
<pre class="r"><code>samples &lt;- posterior_samples(fit, &quot;b_&quot;) %&gt;% 
    mutate(Vo = exp(-b_disc_isold))</code></pre>
<p>We can then plot density curves <span class="citation">(Gabry 2017)</span> for each of the Population-level Effects in our model, including <span class="math inline">\(V_o\)</span>. Figure <a href="#fig:population-density">1</a> shows that our estimate of <span class="math inline">\(V_o\)</span> corresponds very closely to the one reported in the paper (Figure 3 in <span class="citation">Koen et al. (2013)</span>).</p>
<pre class="r"><code>library(bayesplot)
mcmc_areas(samples, point_est = &quot;mean&quot;, prob = .8)</code></pre>
<div class="figure"><span id="fig:population-density"></span>
<img src="/post/2017/2017-10-30-bayesian-estimation-of-signal-detection-theory-models-part-4_files/figure-html/population-density-1.png" alt="Density plots of UVSDT model's Population-level Effects' posterior distributions. Different parameters are indicated on the y-axis, and possible values on the x-axis. Vertical lines are posterior means, and shaded areas are 80\% credible intervals." width="672" />
<p class="caption">
Figure 1: Density plots of UVSDT model’s Population-level Effects’ posterior distributions. Different parameters are indicated on the y-axis, and possible values on the x-axis. Vertical lines are posterior means, and shaded areas are 80% credible intervals.
</p>
</div>
<div id="heterogeneity-parameters" class="section level3">
<h3>Heterogeneity parameters</h3>
<p>Although the “population-level estimates”, which perhaps should be called “average effects”, are usually the main target of inference, they are not the whole story, nor are they necessarily the most interesting part of it. It has been firmly established that, when allowed to vary, the standard deviation of the noise distribution is greater than 1. However, the between-subject variability of this parameter has received less interest. Figure <a href="#fig:population-density2">2</a> reveals that the between-subject heterogeneity of <em>a</em> is quite large: The subject-specific effects have a standard deviation around .5.</p>
<pre class="r"><code>samples_h &lt;- posterior_samples(fit, c(&quot;sd_&quot;, &quot;cor_&quot;))
mcmc_areas(samples_h, point_est = &quot;mean&quot;, prob = .8)</code></pre>
<div class="figure"><span id="fig:population-density2"></span>
<img src="/post/2017/2017-10-30-bayesian-estimation-of-signal-detection-theory-models-part-4_files/figure-html/population-density2-1.png" alt="Density plots of the standard deviation and correlation parameters of the UVSDT model's parameters. Parameter's appended with 'sd_id__' are between-id standard deviations, ones with 'cor_id__' are between-id correlations." width="672" />
<p class="caption">
Figure 2: Density plots of the standard deviation and correlation parameters of the UVSDT model’s parameters. Parameter’s appended with ’sd_id__’ are between-id standard deviations, ones with ’cor_id__’ are between-id correlations.
</p>
</div>
<p>Figure <a href="#fig:population-density2">2</a> also tells us that the subject-specific <em>d’</em>s and <em>a</em>s are correlated ("cor_id__isold__disc_isold"). We can further investigate this relationship by plotting the subject specific signal-SDs and <em>d’</em>s side by side:</p>
<div class="figure"><span id="fig:side-by-side"></span>
<img src="/post/2017/2017-10-30-bayesian-estimation-of-signal-detection-theory-models-part-4_files/figure-html/side-by-side-1.png" alt="Ridgeline plot of posterior distributions of subject-specific standard deviations (left) and d-primes (right). The ordering of subjects on the y-axis is the same, so as to highlight the relationship between the two variables." width="672" />
<p class="caption">
Figure 3: Ridgeline plot of posterior distributions of subject-specific standard deviations (left) and d-primes (right). The ordering of subjects on the y-axis is the same, so as to highlight the relationship between the two variables.
</p>
</div>
<p>As can be seen in the ridgeline plots <span class="citation">(Wilke 2017)</span> in Figure <a href="#fig:side-by-side">3</a>, participants with greater <span class="math inline">\(\sigma_{signal}\)</span> tend to have greater d’: Increase in recognition sensitivity is accompanied with an increase in the signal distribution’s variability. The density plots also make it clear that we are much less certain about individuals whose values (either one) are greater, as shown by the spread out posterior distributions. Yet another way to visualize this relationship is with a scatterplot of the posterior means Figure <a href="#fig:scatterplot">4</a>.</p>
<div class="figure"><span id="fig:scatterplot"></span>
<img src="/post/2017/2017-10-30-bayesian-estimation-of-signal-detection-theory-models-part-4_files/figure-html/scatterplot-1.png" alt="Scatterplot of posterior means of subject-specific d-primes and signal distribution standard deviations." width="672" />
<p class="caption">
Figure 4: Scatterplot of posterior means of subject-specific d-primes and signal distribution standard deviations.
</p>
</div>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Estimating EVSDT and UVSDT models in the Bayesian framework with the brms package <span class="citation">(Bürkner 2017)</span> is both easy (relatively speaking) and informative. In this post, we estimated a hierarchical nonlinear cognitive model using no more than a few lines of code. Previous literature on the topic (e.g. <span class="citation">Rouder et al. (2007)</span>) has focused on simpler (EVSDT) models with more complicated implementations–hopefully in this post I have shown that these models are within the reach of a greater audience, provided that they have some familiarity with R.</p>
<p>Another point worth making is a more general one about hierarchical models: We know that participants introduce (random) variation in our models. Ignoring this variation is clearly not good <span class="citation">(Estes 1956)</span>. It is more appropriate to model this variability, and use the resulting parameters to draw inference about the heterogeneity in parameters (and more generally, cognitive strategies) across individuals. Although maximum likelihood methods provide (noisy) point estimates of what I’ve here called between-subject heterogeneity parameters, the Bayesian method allows drawing firm conclusions about these parameters.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-burkner_brms:_2017">
<p>Bürkner, Paul-Christian. 2017. “Brms: An R Package for Bayesian Multilevel Models Using Stan.” <em>Journal of Statistical Software</em> 80 (1): 1–28. <a href="https://doi.org/10.18637/jss.v080.i01">https://doi.org/10.18637/jss.v080.i01</a>.</p>
</div>
<div id="ref-estes_problem_1956">
<p>Estes, W.K. 1956. “The Problem of Inference from Curves Based on Group Data.” <em>Psychological Bulletin</em> 53 (2): 134–40. <a href="https://doi.org/10.1037/h0045156">https://doi.org/10.1037/h0045156</a>.</p>
</div>
<div id="ref-gabry_bayesplot:_2017">
<p>Gabry, Jonah. 2017. <em>Bayesplot: Plotting for Bayesian Models</em>. <a href="http://mc-stan.org/">http://mc-stan.org/</a>.</p>
</div>
<div id="ref-koen_examining_2013">
<p>Koen, Joshua D., Mariam Aly, Wei-Chun Wang, and Andrew P. Yonelinas. 2013. “Examining the Causes of Memory Strength Variability: Recollection, Attention Failure, or Encoding Variability?” <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 39 (6): 1726–41. <a href="https://doi.org/10.1037/a0033671">https://doi.org/10.1037/a0033671</a>.</p>
</div>
<div id="ref-macmillan_detection_2005">
<p>Macmillan, Neil A., and C. Douglas Creelman. 2005. <em>Detection Theory: A User’s Guide</em>. 2nd ed. Mahwah, N.J: Lawrence Erlbaum Associates.</p>
</div>
<div id="ref-mcnicol_primer_2005">
<p>McNicol, Don. 2005. <em>A Primer of Signal Detection Theory</em>. Psychology Press.</p>
</div>
<div id="ref-r_core_team_r:_2017">
<p>R Core Team. 2017. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-rouder_signal_2007">
<p>Rouder, Jeffrey N., Jun Lu, Dongchu Sun, Paul Speckman, Richard D. Morey, and Moshe Naveh-Benjamin. 2007. “Signal Detection Models with Random Participant and Item Effects.” <em>Psychometrika</em> 72 (4): 621–42. <a href="https://doi.org/10.1007/s11336-005-1350-6">https://doi.org/10.1007/s11336-005-1350-6</a>.</p>
</div>
<div id="ref-singmann_mptinr:_2013">
<p>Singmann, Henrik, and David Kellen. 2013. “MPTinR: Analysis of Multinomial Processing Tree Models in R.” <em>Behavior Research Methods</em> 45 (2): 560–75. <a href="https://doi.org/10.3758/s13428-012-0259-0">https://doi.org/10.3758/s13428-012-0259-0</a>.</p>
</div>
<div id="ref-wilke_ggridges:_2017">
<p>Wilke, Claus O. 2017. <em>Ggridges: Ridgeline Plots in ’Ggplot2’</em>. <a href="https://CRAN.R-project.org/package=ggridges">https://CRAN.R-project.org/package=ggridges</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Recall that intercepts are automatically included, but can be explicitly included by adding <code>1</code> to the formula’s right hand side.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p><code>0 + ...</code> removes the model’s intercept.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
