---
title: How to Compare Two Groups with Robust Bayesian Estimation Using R, Stan and
  brms
author: Matti Vuorre
date: '2017-01-02'
slug: how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms
categories:
  - statistics
tags:
  - bayes
  - R
  - tutorial
  - brms
draft: no
output:
  blogdown::html_page:
    toc: true
    number_sections: false
    toc_depth: 2
summary: "2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups' data."
---


<div id="TOC">
<ul>
<li><a href="#the-t-in-a-t-test">The t in a t-test</a><ul>
<li><a href="#equal-variances-t-test">Equal variances t-test</a></li>
<li><a href="#unequal-variances-t-test">Unequal variances t-test</a></li>
<li><a href="#describing-the-models-underlying-the-t-tests">Describing the model(s) underlying the t-test(s)</a></li>
</ul></li>
<li><a href="#bayesian-estimation-of-the-t-test">Bayesian estimation of the t-test</a><ul>
<li><a href="#equal-variances-model">Equal variances model</a></li>
<li><a href="#unequal-variances-model">Unequal variances model</a></li>
</ul></li>
<li><a href="#robust-bayesian-estimation">Robust Bayesian Estimation</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#further-reading">Further reading</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>Happy New Year 2017 everybody! 2017 will be the year when social scientists finally decided to diversify their applied statistics toolbox, and stop relying 100% on null hypothesis significance testing (NHST). We now recognize that different scientific questions may require different statistical tools, and are ready to adopt new and innovative methods. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups’ data.</p>
<p>More specifically, we’ll focus on the t-test. Everyone knows about it, everyone uses it. Yet, there are (arguably!) better methods for drawing inferences from two independent groups’ metric data (Kruschke, 2013; Morey &amp; Rouder, 2015). Let’s talk about how “<em>Bayesian estimation supersedes the t-test</em>” (Kruschke, 2013).</p>
<p><a href="http://www.indiana.edu/~kruschke/articles/Kruschke2012JEPG.pdf">Kruschke (2013, p.573)</a> writes:</p>
<blockquote>
<p>“When data are interpreted in terms of meaningful parameters in a mathematical description, such as the difference of mean parameters in two groups, it is Bayesian analysis that provides complete information about the credible parameter values. Bayesian analysis is also more intuitive than traditional methods of null hypothesis significance testing (e.g., Dienes, 2011).”</p>
</blockquote>
<p>In that article (<em>Bayesian estimation supersedes the t-test</em>) Kruschke (2013) provided clear and well-reasoned arguments favoring Bayesian parameter estimation over null hypothesis significance testing in the context of comparing two groups, a situation which is usually dealt with a t-test. It also introduced a robust model for comparing two groups, which modeled the data as t-distributed, instead of a Gaussian distribution. The article provided R code for running the estimation procedures, which could be downloaded from the <a href="http://www.indiana.edu/~kruschke/BEST/">author’s website</a> or <a href="https://cran.r-project.org/package=BEST">as an R package</a> (Kruschke &amp; Meredith, 2015).</p>
<p>The R code and programs work well for this specific application (estimating the robust model for one or two groups’ metric data). However, modifying the code to handle more complicated situations is not easy, and the underlying estimation algorithms don’t necessarily scale up to handle more complicated situations. Therefore, today I’ll introduce easy to use, free, open-source, state-of-the-art computer programs for Bayesian estimation, in the context of comparing two groups’ metric (continuous) data. The programs are available for the R programming language–so make sure you are familiar with R basics (e.g. <a href="http://blog.efpsa.org/2016/12/05/introduction-to-data-analysis-using-r/">Vuorre, 2016</a>). I provide R code (it’s super easy, don’t worry!) for t-tests and Bayesian estimation in R using the R package <strong>brms</strong> (Buerkner, 2016), which uses the powerful <a href="http://mc-stan.org/">Stan</a> <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a> program (Stan Development Team, 2016) under the hood.</p>
<p>These programs supersede older Bayesian inference programs because they are easy to use (brms is an interface to Stan, which is actually a programming language in itself), fast, and are able to handle models with thousands of parameters. Learning to implement basic analyses such as t-tests, and Kruschke’s robust model, with these programs is very useful because (obviously) you’ll then be able to do Bayesian statistics in practice, and will be prepared to understand and implement more complex models.</p>
<p>Understanding the results of Bayesian estimation requires understanding some basics of Bayesian statistics, which I won’t describe here at all. If you are not familiar with Bayesian statistics, please read Kruschke’s excellent article (his book is also very good, Kruschke, 2014; see also McElreath, 2016). In fact, you should read the paper anyway, it’s very good.</p>
<p>First, I’ll introduce the basics of t-tests in some detail, and then focus on understanding them as specific instantiations of <em>linear models</em>. If that sounds familiar, skip ahead to <strong>Bayesian Estimation of the t-test</strong>, where I introduce the <strong>brms</strong> package for estimating models using Bayesian methods. Following that, we’ll use “distributional regression” to obtain Bayesian estimates of the unequal variances t-test model. Finally, we’ll learn how to estimate Kruschke’s (2013) BEST model using brms.</p>
<div id="the-t-in-a-t-test" class="section level1">
<h1>The t in a t-test</h1>
<p>We’ll begin with t-tests, using example data from Kruschke’s paper (p. 577):</p>
<blockquote>
<p>“Consider data from two groups of people who take an IQ test. Group 1 (N1=47) consumes a “smart drug,” and Group 2 (N2=42) is a control group that consumes a placebo."</p>
</blockquote>
<p>I’ve decided to call the control group “Group 0”, and the treatment group “Group 1”, because this coding makes it natural to think of the control group as a “reference group”, and any “effect” we’ll estimate will be associated with the treatment group. These data are visualized as histograms, below:</p>
<div class="figure"><span id="fig:dataplot1"></span>
<img src="/post/2017/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms_files/figure-html/dataplot1-1.png" alt="Histograms of the two groups' IQ scores." width="672" />
<p class="caption">
Figure 1: Histograms of the two groups’ IQ scores.
</p>
</div>
<div id="equal-variances-t-test" class="section level2">
<h2>Equal variances t-test</h2>
<p>These two groups’ IQ scores could be compared with a simple <strong>equal variances t-test</strong> (which you shouldn’t use; <a href="https://daniellakens.blogspot.com/2015/01/always-use-welchs-t-test-instead-of.html">Lakens, 2015</a>), also known as Student’s t-test. I have the two groups’ IQ scores in R as two vectors called <code>group_0</code> and <code>group_1</code>, so doing a t-test is as easy as</p>
<pre class="r"><code>t.test(group_0, group_1, var.equal=T)
## 
##  Two Sample t-test
## 
## data:  group_0 and group_1
## t = -1.5587, df = 87, p-value = 0.1227
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.544155  0.428653
## sample estimates:
## mean of x mean of y 
##  100.3571  101.9149</code></pre>
<p>We interpret the t-test in terms of the observed t-value, and whether it exceeds the critical t-value. The critical t-value, in turn, is defined as the extreme <span class="math inline">\(\alpha / 2\)</span> percentiles of a t-distribution with the given degrees of freedom. The current situation is illustrated below:</p>
<div class="figure"><span id="fig:unnamed-chunk-4"></span>
<img src="/post/2017/2017-01-02-how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms_files/figure-html/unnamed-chunk-4-1.png" alt="t distribution with 87 degrees of freedom, and observed t-value. The dashed vertical lines indicate the extreme 2.5 percentiles. We would reject the null hypothesis of no difference if the observed t-value exceeded these percentiles." width="384" />
<p class="caption">
Figure 2: t distribution with 87 degrees of freedom, and observed t-value. The dashed vertical lines indicate the extreme 2.5 percentiles. We would reject the null hypothesis of no difference if the observed t-value exceeded these percentiles.
</p>
</div>
<p>The test results in an observed t-value of 1.56, which is not far enough in the tails of a t-distribution with 87 degrees of freedom to warrant rejecting the null hypothesis (given that we are using <span class="math inline">\(\alpha\)</span> = .05, which may or may not be an entirely brilliant idea (e.g. Rouder, Morey, Verhagen, Province, &amp; Wagenmakers, 2016)). Note that R also reports a 95% CI for the estimated difference between the two groups.</p>
</div>
<div id="unequal-variances-t-test" class="section level2">
<h2>Unequal variances t-test</h2>
<p>Next, we’ll run the more appropriate, <strong>unequal variances t-test</strong> (also known as Welch’s t-test), which R gives by default:</p>
<pre class="r"><code>t.test(group_0, group_1)
## 
##  Welch Two Sample t-test
## 
## data:  group_0 and group_1
## t = -1.6222, df = 63.039, p-value = 0.1098
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.4766863  0.3611848
## sample estimates:
## mean of x mean of y 
##  100.3571  101.9149</code></pre>
<p>Note that while R gives Welch’s t-test by default, SPSS gives both. If you’re using SPSS, make sure to report the Welch’s test results, instead of the equal variances test. Here, the conclusion with respect to rejecting the null hypothesis of equal means is the same. However, notice that the results are numerically different, as they should, because these two t-tests refer to different models!</p>
<p>As a side note, I recently learned that this problem (estimating and testing the difference between two means when the variances are not assumed equal) is unsolved: <a href="https://en.wikipedia.org/wiki/Behrens%E2%80%93Fisher_problem">only approximate solutions are known</a>.</p>
<p>It is of course up to you, as a researcher, to decide whether you assume equal variances or not. But note that we almost always allow the means to be different (that’s the whole point of the test, really), while many treatments may just as well have an effect on the standard deviations.</p>
<p>The first take-home message from today is that there are actually two t-tests, each associated with a different statistical model. And to make clear what the difference is, we must acquaint ourselves with the models.</p>
</div>
<div id="describing-the-models-underlying-the-t-tests" class="section level2">
<h2>Describing the model(s) underlying the t-test(s)</h2>
<p>We don’t usually think of t-<em>tests</em> (and ANOVAs) as <em>models</em>, but it turns out that they are just linear models disguised as tests (see <a href="http://www.sbirc.ed.ac.uk/cyril/SPM-course/Talks/2013/1-GLM-CP.pdf">here</a> and <a href="https://stats.stackexchange.com/questions/59047/how-are-regression-the-t-test-and-the-anova-all-versions-of-the-general-linear">here</a>). Recently, there has been a tremendous push for model/parameter estimation, instead of null hypothesis significance testing (e.g. Cumming, 2014; Kruschke, 2014; see also the brilliant commentary by Gigerenzer, 2004), so we will benefit from thinking about t-tests as linear models. Doing so will facilitate “[interpreting data] in terms of meaningful parameters in a mathematical description” (Kruschke, 2013), and seamlessly expanding our models to handle more complicated situations.</p>
<p>The equal variances t-test models metric data with three parameters: Mean for group A, mean for group B, and one shared standard deviation (i.e. the assumption that the standard deviations [we usually refer to variances, but whatever] are equal between the two groups.)</p>
<p>We call the metric data (IQ scores in our example) <span class="math inline">\(y_{ik}\)</span>, where <span class="math inline">\(i\)</span> is a subscript indicating the <span class="math inline">\(i^{th}\)</span> datum, and <span class="math inline">\(k\)</span> indicates the <span class="math inline">\(k^{th}\)</span> group. So <span class="math inline">\(y_{19, 1}\)</span> would be the 19th datum, belonging to group 1. Then we specify that <span class="math inline">\(y_{ik}\)</span> are Normally distributed, <span class="math inline">\(N(\mu_{ik}, \sigma)\)</span>, where <span class="math inline">\(\mu_{ik}\)</span> indicates the mean of group <span class="math inline">\(k\)</span>, and <span class="math inline">\(\sigma\)</span> the common standard deviation.</p>
<p><span class="math display">\[y_{ik} \sim N(\mu_{ik}, \sigma)\]</span></p>
<p>Read the formula as “Y is normally distributed with mean <span class="math inline">\(\mu_{ik}\)</span> (mu), and standard deviation <span class="math inline">\(\sigma\)</span> (sigma)”. Note that the standard deviation <span class="math inline">\(\sigma\)</span> doesn’t have any subscripts: we assume it is the same for the <span class="math inline">\(k\)</span> groups. Note also that you’ll often see the second parameter in the parentheses as <span class="math inline">\(\sigma^2\)</span>, referring to the variance.</p>
<p>The means for groups 0 and 1 are simply <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\mu_1\)</span>, respectively, and their difference (let’s call it <span class="math inline">\(d\)</span>) is <span class="math inline">\(d = \mu_0 - \mu_1\)</span>. The 95% CI for <span class="math inline">\(d\)</span> is given in the t-test output, and we can tell that it differs from the one given by Welch’s t-test.</p>
<p>It is unsurprising, then, that if we use a different model (the more appropriate unequal variances model; <a href="https://daniellakens.blogspot.com/2015/01/always-use-welchs-t-test-instead-of.html">Lakens, 2015</a>), our inferences may be different. Welch’s t-test is the same as Student’s, except that now we assume (and subsequently estimate) a unique standard deviation <span class="math inline">\(\sigma_{ik}\)</span> for both groups.</p>
<p><span class="math display">\[y_{ik} \sim N(\mu_{ik}, \sigma_{ik})\]</span></p>
<p>This model makes a lot of sense, because rarely are we in a situation to <em>a priori</em> decide that the variance of scores in Group A is equal to the variance of scores in Group B. If you use the equal variances t-test, you should be prepared to justify and defend this assumption. (Deciding between models–such as between these two t-tests–is one way in which our <strong>prior</strong> information enters and influences data analysis. This fact should make you less suspicious about priors in Bayesian analyses.)</p>
<p>Armed with this knowledge, we can now see that “conducting a t-test” can be understood as estimating one of these two models. By estimating the model, we obtain t-values, degrees of freedom, and consequently, <a href="http://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/">p-values</a>.</p>
<p>However, to focus on modeling and estimation, it is easier to think of the t-test as a specific type of the <strong>general linear model</strong>, (aka linear regression). We can re-write the t-test in an equivalent way, but instead have a specific parameter for the difference in means by writing it as a linear model. (For simplicity, I’ll only write the equal variances model):</p>
<p><span class="math display">\[y_{ik} \sim N(\mu_{ik}, \sigma)\]</span>
<span class="math display">\[\mu_{ik} = \beta_0 + \beta_1 Group_{ik}\]</span></p>
<p>Here, <span class="math inline">\(\sigma\)</span> is just as before, but we now <em>model</em> the mean with an intercept (Group 0’s mean, <span class="math inline">\(\beta_0\)</span>) and <em>the effect of Group 1</em> (<span class="math inline">\(\beta_1\)</span>). To understand whats going on, let’s look at the data, <em>Group</em> is an indicator variable in the data, for each row of Group 0’s data <em>Group</em> is zero, and for each row of Group 1’s data <em>Group</em> is one.</p>
<pre><code>##     Group  IQ
## 1       0  99
## 2       0 101
## 3       0 100
## 4       0 101
## ...   ... ...
## 86      1 101
## 87      1 104
## 88      1 100
## 89      1 101</code></pre>
<p>With this model, <span class="math inline">\(\beta_1\)</span> directly tells us the estimated difference in the two groups. And because it is a parameter in the model, it has an associated standard error, t-value, degrees of freedom, and a p-value. This linear model and can be estimated in R with the following line of code:</p>
<pre class="r"><code>olsmod &lt;- lm(IQ ~ Group, data = d)</code></pre>
<p>The key input here is a model formula, which in R is specified as <code>outcome ~ predictor</code> (<code>DV ~ IV</code>). Using the <code>lm()</code> function, we estimated a linear model predicting <code>IQ</code> from an intercept (automatically included) and a Group parameter <code>Group</code>, which is the effect of group 1. I called this object <code>olsmod</code> for Ordinary Least Squares Model.</p>
<p>R has it’s own model formula syntax, which is well worth learning. The formula in the previous model, <code>IQ ~ Group</code> means that we want to regress IQ on an intercept (which is implicitly included), and group (<code>Group</code>). Besides the formula, we only need to provide the data, which is contained in the object I’ve conveniently called <code>d</code>.</p>
<p>You can verify that the results are identical to the equal variances t-test above.</p>
<pre class="r"><code>summary(olsmod)
## 
## Call:
## lm(formula = IQ ~ Group, data = d)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.9149  -0.9149   0.0851   1.0851  22.0851 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 100.3571     0.7263 138.184   &lt;2e-16 ***
## Group         1.5578     0.9994   1.559    0.123    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.707 on 87 degrees of freedom
## Multiple R-squared:  0.02717,    Adjusted R-squared:  0.01599 
## F-statistic:  2.43 on 1 and 87 DF,  p-value: 0.1227</code></pre>
<p>Focus on the <code>Group</code> row in the estimated coefficients. <code>Estimate</code> is the point estimate (best guess) of the difference in means (<span class="math inline">\(d = 101.9149 - 100.3571 = 1.5578\)</span>). <code>t value</code> is the observed t-value (identical to what <code>t.test()</code> reported), and the p-value (<code>Pr(&gt;|t|)</code>) matches as well. The <code>(Intercept)</code> row refers to <span class="math inline">\(\beta_0\)</span>, which is group 0’s mean.</p>
<p>This way of thinking about the model, where we have parameters for one group’s mean, and the effect of the other group, facilitates focusing on the important parameter, the difference, instead of individual means. However, you can of course compute the difference from the means, or the means from one mean and a difference.</p>
</div>
</div>
<div id="bayesian-estimation-of-the-t-test" class="section level1">
<h1>Bayesian estimation of the t-test</h1>
<div id="equal-variances-model" class="section level2">
<h2>Equal variances model</h2>
<p>Next, I’ll illustrate how to estimate the equal variances t-test using Bayesian methods. We use <strong>brms</strong> (Buerkner, 2016), and the familiar R formula syntax which we used with the OLS model.</p>
<p>Estimating this model with R, thanks to the Stan and brms teams (Stan Development Team, 2016; Buerkner, 2016), is as easy as the linear regression model we ran above. If you haven’t yet installed brms, you need to install it first by running <code>install.packages("brms")</code>. Then, to access its functions, load the <strong>brms</strong> package to the current R session.</p>
<pre class="r"><code>library(brms)</code></pre>
<p>The most important function in the brms package is <code>brm()</code>, for Bayesian Regression Model(ing). The user needs only to input a model formula, just as above, and a data frame that contains the variables specified in the formula. <code>brm()</code> then translates the model into Stan language, and asks Stan to compile the model into C++ and estimate it (see Kruschke, 2014; McElreath, 2016 for details about estimation). The result is an R object with the estimated results (and much more). We run the model and save the results to <code>mod_eqvar</code> for equal variances model:</p>
<pre class="r"><code>mod_eqvar &lt;- brm(
  IQ ~ Group, 
  data = d,
  file = here::here(&quot;static/data/iqgroup&quot;)
)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: IQ ~ Group 
##    Data: d (Number of observations: 89) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept   100.36      0.73    98.93   101.77       3934 1.00
## Group         1.56      0.98    -0.41     3.51       3688 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     4.76      0.37     4.10     5.53       4280 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Notice that the model contains three parameters, one of which is the shared standard deviation <code>sigma</code>. Compare the output of the Bayesian model to the one estimated with <code>lm()</code> (OLS):</p>
<table>
<thead>
<tr class="header">
<th align="left">Method</th>
<th align="left">term</th>
<th align="left">estimate</th>
<th align="left">std.error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="left">(Intercept)</td>
<td align="left">100.357</td>
<td align="left">0.726</td>
</tr>
<tr class="even">
<td align="left">OLS</td>
<td align="left">Group</td>
<td align="left">1.558</td>
<td align="left">0.999</td>
</tr>
<tr class="odd">
<td align="left">Bayes</td>
<td align="left">b_Intercept</td>
<td align="left">100.356</td>
<td align="left">0.730</td>
</tr>
<tr class="even">
<td align="left">Bayes</td>
<td align="left">b_Group</td>
<td align="left">1.561</td>
<td align="left">0.981</td>
</tr>
</tbody>
</table>
<p>The point estimates (posterior means in the Bayesian model) and standard errors (SD of the respective posterior distribution) are pretty much identical.</p>
<p>We now know the models behind t-tests, and how to estimate the equal variances t-test using the <code>t.test()</code>, <code>lm()</code>, and <code>brm()</code> functions. We also know how to run Welch’s t-test using <code>t.test()</code>. However, estimating the general linear model version of the unequal variances t-test model is slightly more complicated, because it involves specifying predictors for <span class="math inline">\(\sigma\)</span>, the standard deviation parameter.</p>
</div>
<div id="unequal-variances-model" class="section level2">
<h2>Unequal variances model</h2>
<p>We only need a small adjustment to the equal variances model to specify the unequal variances model:</p>
<p><span class="math display">\[y_{ik} \sim N(\mu_{ik}, \sigma_{ik})\]</span>
<span class="math display">\[\mu_{ik} = \beta_0 + \beta_1 Group_{ik}\]</span></p>
<p>Notice that we now have subscripts for <span class="math inline">\(\sigma\)</span>, denoting that it varies between groups. In fact, we’ll write out a linear model for the standard deviation parameter!</p>
<p><span class="math display">\[\sigma_{ik} = \gamma_0 + \gamma_1 Group_{ik}\]</span></p>
<p>The model now includes, instead of a common <span class="math inline">\(\sigma\)</span>, one parameter for Group 0’s standard deviation <span class="math inline">\(\gamma_0\)</span> (gamma), and one for the effect of Group 1 on the standard deviation <span class="math inline">\(\gamma_1\)</span>, such that group 1’s standard deviation is <span class="math inline">\(\gamma_0 + \gamma_1\)</span>. Therefore, we have 4 free parameters, two means and two standard deviations. (The full specification would include prior distributions for all the parameters, but that topic is outside of the scope of this post.) Let’s estimate!</p>
<p><code>brm()</code> takes more complicated models by wrapping them inside <code>bf()</code> (short for <code>brmsformula()</code>), which is subsequently entered as the first argument to <code>brm()</code>.</p>
<pre class="r"><code>uneq_var_frm &lt;- bf(IQ ~ Group, sigma ~ Group)</code></pre>
<p>You can see that the formula regresses IQ on Group, such that we’ll have an intercept (implicitly included), and an effect of Group 1. Remarkably, we are also able to model the standard deviation sigma, and we regress it on Group (it will also have an intercept and effect of group).</p>
<pre class="r"><code>mod_uneqvar &lt;- brm(
  uneq_var_frm, 
  data = d, 
  cores=4,
  file = here::here(&quot;static/data/iqgroup-uv&quot;)
)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = log 
## Formula: IQ ~ Group 
##          sigma ~ Group
##    Data: d (Number of observations: 89) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept         100.36      0.38    99.59   101.10       4895 1.00
## sigma_Intercept     0.93      0.11     0.72     1.16       4293 1.00
## Group               1.54      1.03    -0.49     3.53       2273 1.00
## sigma_Group         0.88      0.15     0.57     1.16       4036 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>The model’s output contains our 4 parameters. <code>Intercept</code> is the mean for group 0, <code>Group 1</code> is the “effect of group 1”. The <code>sigma_Intercept</code> is the standard deviation of Group 0, <code>sigma_Group</code> is the effect of group 1 on the standard deviation (the SD of Group 1 is <code>sigma_Intercept</code> + <code>sigma_Group</code>). The sigmas are implicitly modeled through a log-link (because they must be positive). To convert them back to the scale of the data, they need to be exponentiated. After taking the exponents of the sigmas, the results look like this:</p>
<table>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">Estimate</th>
<th align="left">Est.Error</th>
<th align="left">l-95% CI</th>
<th align="left">u-95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">b_sigma_Group</td>
<td align="left">2.43</td>
<td align="left">0.38</td>
<td align="left">1.77</td>
<td align="left">3.20</td>
</tr>
<tr class="even">
<td align="left">b_Intercept</td>
<td align="left">100.36</td>
<td align="left">0.38</td>
<td align="left">99.59</td>
<td align="left">101.10</td>
</tr>
<tr class="odd">
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="even">
<td align="left">b_sigma_Intercept</td>
<td align="left">1.74</td>
<td align="left">0.84</td>
<td align="left">0.75</td>
<td align="left">3.08</td>
</tr>
</tbody>
</table>
<p>For comparison, here is the “observed SD” of group 0:</p>
<pre><code>## [1] 2.52</code></pre>
<p>Keep in mind that the parameters refer to Group 0’s mean (Intercept) and SD (sigma), and the difference between groups in those values (Group) and (sigma_Group). We now have fully Bayesian estimates of the 4 parameters of the unequal variances t-test model. Because p-values have no place in Bayesian inference, they are not reported in the output. However, you can calculate a quantity that is equivalent to a one-sided p-value from the posterior distribution: Take the proportion of posterior density (MCMC samples) above/below a reference value (0). This is definitely not the most useful thing you can do with a posterior distribution, but the fact that it numerically matches a one-sided p-value is quite interesting:</p>
<pre class="r"><code># Posterior distribution of Group effect
x &lt;- as.data.frame(mod_uneqvar, pars = &quot;b_Group&quot;)[,1]
# Proportion of MCMC samples below zero
round((sum(x &lt; 0) / length(x)), 3)
## [1] 0.068
# One sided p-value from t-test
round(t.test(group_0, group_1, data = d, alternative = &quot;less&quot;)$p.value, 3)
## [1] 0.055</code></pre>
<p>I’m showing this remarkable fact (Marsman &amp; Wagenmakers, no date) not to persuade you to stick with p-values, but to alleviate fears that these methods would always produce discrepant results.</p>
<p>Although this model is super easy to estimate with <code>brm()</code> (which, I should emphasize, uses Stan for the estimation procedures), the model seems, frankly speaking, strange. I am just not used to modeling variances, and I’ll bet a quarter that neither are you. Nevertheless, there it is!</p>
<p>Finally, let’s move on to Kruschke’s (2013) “Robust Bayesian Estimation” model.</p>
</div>
</div>
<div id="robust-bayesian-estimation" class="section level1">
<h1>Robust Bayesian Estimation</h1>
<p>Kruschke’s robust model is a comparison of two groups, using five parameters: One mean for each group, one standard deviation for each group, just as in the unequal variances model above. The fifth parameter is a “normality” parameter, <span class="math inline">\(\nu\)</span> (nu), which means that we are now using a t-distribution to model the data. Using a t-distribution to model the data, instead of a Gaussian, means that the model (and therefore our inferences) are less sensitive to extreme values (outliers). Here’s what the model looks like:</p>
<p><span class="math display">\[y_{ik} \sim T(\nu, \mu_{ik}, \sigma_{ik})\]</span></p>
<p>Read the above formula as “Y are random draws from a t-distribution with ‘normality’ parameter <span class="math inline">\(\nu\)</span>, mean <span class="math inline">\(\mu_{ik}\)</span>, and standard deviation <span class="math inline">\(\sigma_{ik}\)</span>”. We have a linear model for the means and standard deviations:</p>
<p><span class="math display">\[\mu_{ik} = \beta_0 + \beta_1 Group_{ik}\]</span></p>
<p>and</p>
<p><span class="math display">\[\sigma_{ik} = \gamma_0 + \gamma_1 Group_{ik}\]</span></p>
<p>This model, as you can see, is almost identical to the unequal variances t-test, but instead uses a t distribution (we assume data are t-distributed), and includes the normality parameter. Using <code>brm()</code> we can still use the unequal variances model, but have to specify the t-distribution. We do this by specifying the <code>family</code> argument to be <code>student</code> (as in Student’s t)</p>
<pre class="r"><code>mod_robust &lt;- brm(
  bf(IQ ~ Group, sigma ~ Group),
  family=student,
  data = d, 
  cores=4,
  file = here::here(&quot;static/data/iqgroup-robust&quot;)
  )</code></pre>
<pre><code>##  Family: student 
##   Links: mu = identity; sigma = log; nu = identity 
## Formula: IQ ~ Group 
##          sigma ~ Group
##    Data: d (Number of observations: 89) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## Intercept        100.521     0.206  100.140  100.925       6459 0.999
## sigma_Intercept    0.002     0.194   -0.380    0.380       4289 1.001
## Group              1.033     0.430    0.198    1.885       2701 1.001
## sigma_Group        0.674     0.252    0.185    1.183       4111 1.000
## 
## Family Specific Parameters: 
##    Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## nu    1.867     0.481    1.170    3.061       3110 1.001
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>The effect of Group is about one unit, with a 95% Credible Interval from 0.2 to 1.8.</p>
<p>Finally, let’s compare the results to those in Kruschke’s paper (2013, p.578). Before we do this, I’ll convert the estimated parameters to means and standard deviations (instead of the “regression effects” produced by default.) Recall that I recoded the group labels used by Kruschke in the paper, what he calls group 2 is group 0 (control group) in our analyses, but group 1 is still group 1. In the following I transform the results and compute HDIs to obtain results most compatible with Kruschke:</p>
<table>
<thead>
<tr class="header">
<th align="left">key</th>
<th align="left">m</th>
<th align="left">HDIlwr</th>
<th align="left">HDIupr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">group0_mean</td>
<td align="left">100.52</td>
<td align="left">100.14</td>
<td align="left">100.92</td>
</tr>
<tr class="even">
<td align="left">group1_mean</td>
<td align="left">101.55</td>
<td align="left">100.78</td>
<td align="left">102.27</td>
</tr>
<tr class="odd">
<td align="left">diff_means</td>
<td align="left">1.03</td>
<td align="left">0.16</td>
<td align="left">1.83</td>
</tr>
<tr class="even">
<td align="left">group0_sigma</td>
<td align="left">1.02</td>
<td align="left">0.66</td>
<td align="left">1.43</td>
</tr>
<tr class="odd">
<td align="left">group1_sigma</td>
<td align="left">2.01</td>
<td align="left">1.28</td>
<td align="left">2.89</td>
</tr>
<tr class="even">
<td align="left">diff_sigmas</td>
<td align="left">2.03</td>
<td align="left">1.10</td>
<td align="left">3.07</td>
</tr>
<tr class="odd">
<td align="left">nu</td>
<td align="left">1.87</td>
<td align="left">1.05</td>
<td align="left">2.78</td>
</tr>
<tr class="even">
<td align="left">nulog10</td>
<td align="left">0.26</td>
<td align="left">0.06</td>
<td align="left">0.47</td>
</tr>
</tbody>
</table>
<p>Notice that Kruschke reports modes (2013, p. 578), but our point estimates are means. The results with respect to the group means are identical to two decimal points; the standard deviations are slightly more discrepant, because the paper reports modes, but we focus on posterior means.</p>
<p>Finally, here is how to estimate the model using the original code (Kruschke &amp; Meredith, 2015):</p>
<pre class="r"><code>library(BEST)
BEST &lt;- BESTmcmc(group_0, group_1)
BEST
## MCMC fit results for BEST analysis:
## 100002 simulations saved.
##           mean     sd  median   HDIlo   HDIup Rhat n.eff
## mu1    100.525 0.2126 100.524 100.118 100.948    1 61439
## mu2    101.551 0.3791 101.552 100.814 102.309    1 60570
## nu       1.840 0.4730   1.766   1.032   2.744    1 24291
## sigma1   1.051 0.2072   1.032   0.681   1.473    1 35896
## sigma2   2.059 0.4294   2.020   1.257   2.909    1 29402
## 
## &#39;HDIlo&#39; and &#39;HDIup&#39; are the limits of a 95% HDI credible interval.
## &#39;Rhat&#39; is the potential scale reduction factor (at convergence, Rhat=1).
## &#39;n.eff&#39; is a crude measure of effective sample size.</code></pre>
<p>This output reports posterior means and HDI limits, which we report above. You can verify that they match very closely to each other. This <code>BESTmcmc()</code> function is great, but with brms you are able to estimate a vast variety of models.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Well, that ended up much longer than what I intended. The aim was both to illustrate the ease of Bayesian modeling in R using <strong>brms</strong> (Buerkner, 2016) and <strong>Stan</strong> (Stan Development Team, 2016), and highlight the fact that we can easily move from simple t-tests to more complex (and possibly better) models.</p>
<p>If you’ve followed through, you should be able to conduct Student’s (equal variances) and Welch’s (unequal variances) t-tests in R, and to think about those tests as instantiations of general linear models. Further, you should be able to estimate these models using Bayesian methods.</p>
<p>You should now also be familiar with Kruschke’s robust model for comparing two groups’ metric data, and be able to implement it in one line of R code. This model was able to find credible differences between two groups, although the frequentist t-tests and models reported p-values well above .05. That should be motivation enough to try robust (Bayesian) models on your own data.</p>
</div>
<div id="further-reading" class="section level1">
<h1>Further reading</h1>
<p>I didn’t take any space here to discuss the interpretation of Bayesian statistics. For this, I recommend Kruschke (2014), McElreath (2016). See also Etz, Gronau, Dablander, Edelsbrunner, &amp; Baribault (2016) for an introduction to Bayesian statistics.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<div class="csl-bib-body" style="line-height: 2; padding-left: 2em; text-indent:-2em;">
<div class="csl-entry">
Buerkner, P.-C. (2016). <i>brms: Bayesian Regression Models using Stan</i>. Retrieved from <a href="http://CRAN.R-project.org/package=brms" class="uri">http://CRAN.R-project.org/package=brms</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=computerProgram&amp;rft.title=brms%3A%20Bayesian%20Regression%20Models%20using%20Stan&amp;rft.identifier=http%3A%2F%2FCRAN.R-project.org%2Fpackage%3Dbrms&amp;rft.aufirst=Paul-Christian&amp;rft.aulast=Buerkner&amp;rft.au=Paul-Christian%20Buerkner&amp;rft.date=2016"></span>
<div class="csl-entry">
Cumming, G. (2014). The New Statistics Why and How. <i>Psychological Science</i>, <i>25</i>(1), 7–29. <a href="https://doi.org/10.1177/0956797613504966" class="uri">https://doi.org/10.1177/0956797613504966</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1177%2F0956797613504966&amp;rft_id=info%3Apmid%2F24220629&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The%20New%20Statistics%20Why%20and%20How&amp;rft.jtitle=Psychological%20Science&amp;rft.stitle=Psychological%20Science&amp;rft.volume=25&amp;rft.issue=1&amp;rft.aufirst=Geoff&amp;rft.aulast=Cumming&amp;rft.au=Geoff%20Cumming&amp;rft.date=2014-01-01&amp;rft.pages=7-29&amp;rft.spage=7&amp;rft.epage=29&amp;rft.issn=0956-7976%2C%201467-9280&amp;rft.language=en"></span>
<div class="csl-entry">
Dienes, Z. (2011). Bayesian Versus Orthodox Statistics: Which Side Are You On? <i>Perspectives on Psychological Science</i>, <i>6</i>(3), 274–290. <a href="https://doi.org/10.1177/1745691611406920" class="uri">https://doi.org/10.1177/1745691611406920</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1177%2F1745691611406920&amp;rft_id=info%3Apmid%2F26168518&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Bayesian%20Versus%20Orthodox%20Statistics%3A%20Which%20Side%20Are%20You%20On%3F&amp;rft.jtitle=Perspectives%20on%20Psychological%20Science&amp;rft.stitle=Perspectives%20on%20Psychological%20Science&amp;rft.volume=6&amp;rft.issue=3&amp;rft.aufirst=Zoltan&amp;rft.aulast=Dienes&amp;rft.au=Zoltan%20Dienes&amp;rft.date=2011-05-01&amp;rft.pages=274-290&amp;rft.spage=274&amp;rft.epage=290&amp;rft.issn=1745-6916%2C%201745-6924&amp;rft.language=en"></span>
<div class="csl-entry">
Etz, A., Gronau, Q. F., Dablander, F., Edelsbrunner, P. A., &amp; Baribault, B. (2016). How to become a Bayesian in eight easy steps: An annotated reading list. <i>ResearchGate</i>. Retrieved from <a href="https://www.researchgate.net/publication/301981861_How_to_become_a_Bayesian_in_eight_easy_steps_An_annotated_reading_list" class="uri">https://www.researchgate.net/publication/301981861_How_to_become_a_Bayesian_in_eight_easy_steps_An_annotated_reading_list</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=How%20to%20become%20a%20Bayesian%20in%20eight%20easy%20steps%3A%20An%20annotated%20reading%20list&amp;rft.jtitle=ResearchGate&amp;rft.aufirst=Alexander&amp;rft.aulast=Etz&amp;rft.au=Alexander%20Etz&amp;rft.au=Quentin%20F.%20Gronau&amp;rft.au=Fabian%20Dablander&amp;rft.au=Peter%20A.%20Edelsbrunner&amp;rft.au=Beth%20Baribault&amp;rft.date=2016-05-06"></span>
<div class="csl-entry">
Kruschke, J. K. (2013). Bayesian estimation supersedes the t test. <i>Journal of Experimental Psychology: General</i>, <i>142</i>(2), 573–603. <a href="https://doi.org/10.1037/a0029146" class="uri">https://doi.org/10.1037/a0029146</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1037%2Fa0029146&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Bayesian%20estimation%20supersedes%20the%20t%20test.&amp;rft.jtitle=Journal%20of%20Experimental%20Psychology%3A%20General&amp;rft.volume=142&amp;rft.issue=2&amp;rft.aufirst=John%20K.&amp;rft.aulast=Kruschke&amp;rft.au=John%20K.%20Kruschke&amp;rft.date=2013&amp;rft.pages=573-603&amp;rft.spage=573&amp;rft.epage=603&amp;rft.issn=1939-2222%2C%200096-3445&amp;rft.language=en"></span>
<div class="csl-entry">
Kruschke, J. K. (2014). <i>Doing Bayesian Data Analysis: A Tutorial Introduction with R</i> (2nd Edition). Burlington, MA: Academic Press.
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-0-12-381486-9&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Doing%20Bayesian%20Data%20Analysis%3A%20A%20Tutorial%20Introduction%20with%20R&amp;rft.place=Burlington%2C%20MA&amp;rft.publisher=Academic%20Press&amp;rft.edition=2nd%20Edition&amp;rft.aufirst=John%20K.&amp;rft.aulast=Kruschke&amp;rft.au=John%20K.%20Kruschke&amp;rft.date=2014&amp;rft.tpages=673&amp;rft.isbn=978-0-12-381486-9&amp;rft.language=en"></span>
<div class="csl-entry">
Lakens, D. (2015, January 26). The 20% Statistician: Always use Welch’s t-test instead of Student’s t-test. Retrieved from <a href="https://daniellakens.blogspot.com/2015/01/always-use-welchs-t-test-instead-of.html" class="uri">https://daniellakens.blogspot.com/2015/01/always-use-welchs-t-test-instead-of.html</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=The%2020%25%20Statistician%3A%20Always%20use%20Welch&#39;s%20t-test%20instead%20of%20Student&#39;s%20t-test&amp;rft.identifier=https%3A%2F%2Fdaniellakens.blogspot.com%2F2015%2F01%2Falways-use-welchs-t-test-instead-of.html&amp;rft.aufirst=Daniel&amp;rft.aulast=Lakens&amp;rft.au=Daniel%20Lakens&amp;rft.date=2015-01-26"></span>
<div class="csl-entry">
Marsman, M., &amp; Wagenmakers, E.-J. (no date). Three Insights from a Bayesian Interpretation of the One-Sided P Value. Retrieved from <a href="http://www.ejwagenmakers.com/inpress/MarsmanWagenmakersOneSidedPValue.pdf" class="uri">http://www.ejwagenmakers.com/inpress/MarsmanWagenmakersOneSidedPValue.pdf</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Three%20Insights%20from%20a%20Bayesian%20Interpretation%20of%20the%20One-Sided%20P%20Value&amp;rft.aufirst=Maarten&amp;rft.aulast=Marsman&amp;rft.au=Maarten%20Marsman&amp;rft.au=Eric-Jan%20Wagenmakers"></span>
<div class="csl-entry">
McElreath, R. (2016). <i>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</i>. CRC Press.
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-4822-5346-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Statistical%20Rethinking%3A%20A%20Bayesian%20Course%20with%20Examples%20in%20R%20and%20Stan&amp;rft.publisher=CRC%20Press&amp;rft.aufirst=Richard&amp;rft.aulast=McElreath&amp;rft.au=Richard%20McElreath&amp;rft.date=2016-01-05&amp;rft.tpages=485&amp;rft.isbn=978-1-4822-5346-7&amp;rft.language=en"></span>
<div class="csl-entry">
Morey, R. D., &amp; Rouder, J. (2015). <i>BayesFactor: Computation of Bayes Factors for Common Designs</i>. Retrieved from <a href="https://CRAN.R-project.org/package=BayesFactor" class="uri">https://CRAN.R-project.org/package=BayesFactor</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=BayesFactor%3A%20Computation%20of%20Bayes%20Factors%20for%20Common%20Designs&amp;rft.aufirst=Richard%20D.&amp;rft.aulast=Morey&amp;rft.au=Richard%20D.%20Morey&amp;rft.au=Jeffrey%20Rouder&amp;rft.date=2015"></span>
<div class="csl-entry">
Rouder, J. N., Morey, R. D., Verhagen, J., Province, J. M., &amp; Wagenmakers, E.-J. (2016). Is There a Free Lunch in Inference? <i>Topics in Cognitive Science</i>, <i>8</i>(3), 520–547. <a href="https://doi.org/10.1111/tops.12214" class="uri">https://doi.org/10.1111/tops.12214</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1111%2Ftops.12214&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Is%20There%20a%20Free%20Lunch%20in%20Inference%3F&amp;rft.jtitle=Topics%20in%20Cognitive%20Science&amp;rft.stitle=Top%20Cogn%20Sci&amp;rft.volume=8&amp;rft.issue=3&amp;rft.aufirst=Jeffrey%20N.&amp;rft.aulast=Rouder&amp;rft.au=Jeffrey%20N.%20Rouder&amp;rft.au=Richard%20D.%20Morey&amp;rft.au=Josine%20Verhagen&amp;rft.au=Jordan%20M.%20Province&amp;rft.au=Eric-Jan%20Wagenmakers&amp;rft.date=2016-07-01&amp;rft.pages=520-547&amp;rft.spage=520&amp;rft.epage=547&amp;rft.issn=1756-8765&amp;rft.language=en"></span>
<div class="csl-entry">
Stan Development Team. (2016). <i>Stan: A C++ Library for Probability and Sampling, Version 2.14.1</i>. Retrieved from <a href="http://mc-stan.org/" class="uri">http://mc-stan.org/</a>
</div>
<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Stan%3A%20A%20C%2B%2B%20Library%20for%20Probability%20and%20Sampling%2C%20Version%202.14.1&amp;rft.aulast=Stan%20Development%20Team&amp;rft.au=Stan%20Development%20Team&amp;rft.date=2016"></span>
<div class="csl-entry">
Vuorre, M. (2016, December 5). Introduction to Data Analysis using R. Retrieved from <a href="http://blog.efpsa.org/2016/12/05/introduction-to-data-analysis-using-r/" class="uri">http://blog.efpsa.org/2016/12/05/introduction-to-data-analysis-using-r/</a>
</div>
<p><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Introduction%20to%20Data%20Analysis%20using%20R&amp;rft.description=R%20is%20a%20statistical%20programming%20language%20whose%20popularity%20is%20quickly%20overtaking%20SPSS%20and%20other%20%E2%80%9Ctraditional%E2%80%9D%20point-and-click%20software%20packages%20(Muenchen%2C%202015).%20But%20why%20would%20anyone%20use%20a%20programmin%E2%80%A6&amp;rft.identifier=http%3A%2F%2Fblog.efpsa.org%2F2016%2F12%2F05%2Fintroduction-to-data-analysis-using-r%2F&amp;rft.aufirst=Matti&amp;rft.aulast=Vuorre&amp;rft.au=Matti%20Vuorre&amp;rft.date=2016-12-05"></span></p>
</div>
</div>
